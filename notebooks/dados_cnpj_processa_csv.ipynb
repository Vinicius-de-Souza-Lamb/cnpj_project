{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  Imports & global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú REPO_ROOT      ‚Üí /Users/viniciuslamb/Documents/portifolio_cnpj\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú SOURCE_ZIP_DIR ‚Üí /Users/viniciuslamb/Documents/portifolio_cnpj/airflow/datasets/public-zips\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú CSV_DIR        ‚Üí /Users/viniciuslamb/Documents/portifolio_cnpj/airflow/datasets/public-data\n"
     ]
    }
   ],
   "source": [
    "# Globals ‚Äì shared by every other cell\n",
    "from __future__ import annotations\n",
    "\n",
    "import io, logging, os, time, zipfile\n",
    "from pathlib import Path\n",
    "from typing  import Dict, List, Optional\n",
    "\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from postgre_connector import Postgres  # import the Postgres helper\n",
    "\n",
    "load_dotenv()                            # .env ‚Üí os.environ\n",
    "\n",
    "# ‚îÄ‚îÄ Project folders ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Automatically locate the main repository root\n",
    "# The loop walks upward until both 'airflow/dags' and 'streamlit_app'\n",
    "# directories are found ‚Äî this ensures we‚Äôre inside the real project,\n",
    "# not inside a nested notebook folder.\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "while REPO_ROOT != REPO_ROOT.parent:\n",
    "    has_airflow = (REPO_ROOT / \"airflow\" / \"dags\").is_dir()\n",
    "    has_streamlit = (REPO_ROOT / \"streamlit_app\").is_dir()\n",
    "    if has_airflow and has_streamlit:\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# Define dataset directories used by Airflow\n",
    "BASE_DIR       = REPO_ROOT / \"airflow\" / \"datasets\"\n",
    "SOURCE_ZIP_DIR = BASE_DIR / \"public-zips\"\n",
    "CSV_DIR        = BASE_DIR / \"public-data\"\n",
    "\n",
    "# Ensure the dataset folders exist before loading or writing files\n",
    "for p in (SOURCE_ZIP_DIR, CSV_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ logging ‚Äì INFO by default ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s  %(levelname)-8s  %(name)s ‚ûú %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"nb-cnpj-ingest\")\n",
    "log.info(\"REPO_ROOT      ‚Üí %s\", REPO_ROOT)\n",
    "log.info(\"SOURCE_ZIP_DIR ‚Üí %s\", SOURCE_ZIP_DIR)\n",
    "log.info(\"CSV_DIR        ‚Üí %s\", CSV_DIR)\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ column layouts (Portuguese names) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "LAYOUTS: Dict[str, List[str]] = {\n",
    "    \"header\": [\n",
    "        \"tipo_do_registro\", \"filler\", \"nome_do_arquivo\", \"data_de_gravacao\",\n",
    "        \"numero_da_remessa\", \"filler2\", \"fim_de_registro\"\n",
    "    ],\n",
    "    \"cnae\": [\"codigo\", \"descricao\"],\n",
    "    \"empresas\": [\n",
    "        \"cnpj_basico\", \"razao_social\", \"natureza_juridica\",\n",
    "        \"qualificacao_responsavel\", \"capital_social_str\", \"porte_empresa\",\n",
    "        \"ente_federativo_responsavel\"\n",
    "    ],\n",
    "    \"estabelecimento\": [\n",
    "        \"cnpj_basico\", \"cnpj_ordem\", \"cnpj_dv\", \"matriz_filial\", \n",
    "        \"nome_fantasia\", \"situacao_cadastral\", \"data_situacao_cadastral\",\n",
    "        \"motivo_situacao_cadastral\", \"nome_cidade_exterior\", \"pais\",\n",
    "        \"data_inicio_atividades\", \"cnae_fiscal\", \"cnae_fiscal_secundaria\",\n",
    "        \"tipo_logradouro\", \"logradouro\", \"numero\", \"complemento\", \n",
    "        \"bairro\", \"cep\", \"uf\", \"municipio\", \"ddd1\", \"telefone1\", \n",
    "        \"ddd2\", \"telefone2\", \"ddd_fax\", \"fax\", \"correio_eletronico\", \n",
    "        \"situacao_especial\", \"data_situacao_especial\"\n",
    "    ],\n",
    "    \"motivo\": [\"codigo\", \"descricao\"],\n",
    "    \"municipio\": [\"codigo\", \"descricao\"],\n",
    "    \"natureza_juridica\": [\"codigo\", \"descricao\"],\n",
    "    \"pais\": [\"codigo\", \"descricao\"],\n",
    "    \"qualificacao_socio\": [\"codigo\", \"descricao\"],\n",
    "    \"simples\": [\n",
    "        \"cnpj_basico\", \"opcao_simples\", \"data_opcao_simples\",\n",
    "        \"data_exclusao_simples\", \"opcao_mei\", \"data_opcao_mei\",\n",
    "        \"data_exclusao_mei\"\n",
    "    ],\n",
    "    \"socios_original\": [\n",
    "        \"cnpj_basico\", \"identificador_de_socio\", \"nome_socio\", \n",
    "        \"cnpj_cpf_socio\", \"qualificacao_socio\", \"data_entrada_sociedade\", \n",
    "        \"pais\", \"representante_legal\", \"nome_representante\", \n",
    "        \"qualificacao_representante_legal\", \"faixa_etaria\"\n",
    "    ],\n",
    "    \"cnaes_secundarias\": [\n",
    "        \"tipo_do_registro\", \"indicador_full_diario\", \"tipo_de_atualizacao\",\n",
    "        \"cnpj\", \"cnae_secundaria\", \"filler\", \"fim_de_registro\"\n",
    "    ],\n",
    "    \"trailler\": [\n",
    "        \"tipo_do_registro\", \"filler\", \"total_de_registros_t1\", \"total_de_registros_t2\",\n",
    "        \"total_de_registros_t3\", \"total_de_registros\", \"filler2\", \"fim_de_registro\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)  ZIP ‚Üí CSV  (pure extraction, no transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 1¬∑a)  ZIP ‚Üí CSV extractor (pure ‚Äúdump‚Äù, zero transformation)\n",
    "# ------------------------------------------------------------------ #\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def extract_zip_to_csv(limit: Optional[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Decompress every ``*.zip`` found under :pydata:`SOURCE_ZIP_DIR`\n",
    "    and write the *verbatim* contents to :pydata:`CSV_DIR`.\n",
    "\n",
    "    One archive may contain multiple files ‚Äì each gets an output name\n",
    "    like:\n",
    "\n",
    "        {zip_stem}_{inner_filename}.csv\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> extract_zip_to_csv()        # process everything\n",
    "    >>> extract_zip_to_csv(limit=2) # process only the first 2 ZIPs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    limit : int | None, optional\n",
    "        Maximum number of ZIP archives to process.  Passing *None*\n",
    "        (default) means ‚Äúprocess them all‚Äù.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * **No** data transformation happens here ‚Äì we copy the bytes 1-for-1.\n",
    "    * Already-extracted files are skipped (idempotent behaviour).\n",
    "    * Detailed logging shows progress and per-file timing.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 0)  Gather and optionally truncate the list of ZIP archives\n",
    "    # ----------------------------------------------------------------\n",
    "    zip_paths = sorted(\n",
    "        p for p in SOURCE_ZIP_DIR.iterdir()\n",
    "        if p.suffix.lower() == \".zip\"\n",
    "    )\n",
    "\n",
    "    if not zip_paths:\n",
    "        log.warning(\"‚ö†Ô∏è  No ZIP archives found in %s ‚Äì nothing to extract.\", SOURCE_ZIP_DIR)\n",
    "        return\n",
    "\n",
    "    if limit is not None:\n",
    "        zip_paths = zip_paths[:limit]                    # truncate for quick tests\n",
    "\n",
    "    total_archives = len(zip_paths)\n",
    "    processed      = 0\n",
    "    t_global_start = time.perf_counter()\n",
    "\n",
    "    log.info(\"üîß  ZIP ‚ûú CSV extraction started (archives=%s)\", total_archives)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 1)  Walk each .zip and dump everything inside\n",
    "    # ----------------------------------------------------------------\n",
    "    for idx, zip_path in enumerate(zip_paths, start=1):\n",
    "        t_start = time.perf_counter()\n",
    "        log.info(\"‚Üí [%s/%s]  %s\", idx, total_archives, zip_path.name)\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path) as zf:\n",
    "                for inner_name in zf.namelist():\n",
    "                    # Build the output filename:  {zip_stem}_{inner_basename}.csv\n",
    "                    out_path = CSV_DIR / f\"{zip_path.stem}_{Path(inner_name).name}.csv\"\n",
    "\n",
    "                    # Skip if we already have it ‚Äì idempotency is ‚ù§Ô∏è\n",
    "                    if out_path.exists():\n",
    "                        log.debug(\"   ‚Ä¢ skip (exists)  %s\", out_path.name)\n",
    "                        continue\n",
    "\n",
    "                    # Copy the bytes verbatim (no decoding here)\n",
    "                    with zf.open(inner_name) as src, out_path.open(\"wb\") as dst:\n",
    "                        dst.write(src.read())\n",
    "\n",
    "                    log.debug(\"   ‚Ä¢ extracted        %s\", out_path.name)\n",
    "\n",
    "            processed += 1\n",
    "            log.info(\"   ‚úî  done in %.2fs\", time.perf_counter() - t_start)\n",
    "\n",
    "        except zipfile.BadZipFile as err:\n",
    "            # Corrupted archive ‚Äì log and carry on\n",
    "            log.error(\"‚ùå  Bad ZIP %s ‚Äì %s\", zip_path.name, err)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 2)  Global summary\n",
    "    # ----------------------------------------------------------------\n",
    "    total_time = time.perf_counter() - t_global_start\n",
    "    log.info(\"üèÅ  Extraction finished: %s / %s archive(s) processed in %.2fs\",\n",
    "             processed, total_archives, total_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)  Identify which layout a CSV belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 3)  Identify which layout a CSV belongs to\n",
    "# ------------------------------------------------------------------ #\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def layout_key(file_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Inspect *file_name* and return the matching entry inside\n",
    "    the global :pydata:`LAYOUTS` dict.\n",
    "\n",
    "    The match is intentionally **simple**:  \n",
    "    we look for each layout keyword (``\"empresas\"``, ``\"cnae\"``,\n",
    "    ``\"estabelecimento\"`` ‚Ä¶) as a *substring* of the file name\n",
    "    ‚Äì case-insensitive.  The **first** hit wins.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Any file name or full path (only the final component is used).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str | None\n",
    "        * The layout key (e.g. ``\"empresas\"``) when a match is found;\n",
    "        * **None** when no keyword matches.\n",
    "    \"\"\"\n",
    "    # Normalize: keep just the final component and lower-case it\n",
    "    fname_low = Path(file_name).name.lower()\n",
    "\n",
    "    # 1) Direct match against our canonical keys in LAYOUTS\n",
    "    #    (layout keys already reflect business names we use in tables)\n",
    "    for key in LAYOUTS:                  # LAYOUTS is defined at module top\n",
    "        if key in fname_low:\n",
    "            return key                   # ‚Üê success\n",
    "\n",
    "    # 2) Common aliases found in Receita filenames (abbreviations)\n",
    "    #    We keep this list local to the function for clarity/encapsulation.\n",
    "    #    Order matters only if two aliases could match the same file name.\n",
    "    aliases = {\n",
    "        # canonical_key              # examples of filename substrings\n",
    "        \"natureza_juridica\": [\"natju\", \"natureza\"],\n",
    "        \"qualificacao_socio\": [\"quals\", \"qualific\"],\n",
    "        \"socios_original\":    [\"socio\", \"socios\"],\n",
    "        \"estabelecimento\":    [\"estabelec\", \"estab\"],\n",
    "        \"empresas\":           [\"empre\", \"empresa\", \"empresas\"],\n",
    "        \"cnae\":               [\"cnae\"],\n",
    "        \"motivo\":             [\"motic\", \"motivo\"],\n",
    "        \"municipio\":          [\"munic\", \"municipio\"],\n",
    "        \"pais\":               [\"pais\"],\n",
    "        \"cnaes_secundarias\":  [\"secundaria\", \"secundarias\"],\n",
    "        \"trailler\":           [\"trailler\", \"trailer\"],\n",
    "        \"header\":             [\"header\", \"cabecalho\"],\n",
    "    }\n",
    "\n",
    "    for canonical, subs in aliases.items():\n",
    "        if any(sub in fname_low for sub in subs):\n",
    "            return canonical\n",
    "\n",
    "    # Fallback ‚Äì unknown file\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)  Create *typed* temp tables (one per CNPJ layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 4)  Create *typed* temp tables (one per CNPJ layout)\n",
    "# ------------------------------------------------------------------ #\n",
    "def create_temp_tables() -> None:\n",
    "    \"\"\"\n",
    "    Build/rebuild every **temp_*** table in PostgreSQL using the\n",
    "    *true* data-types published by Receita Federal.  \n",
    "    All DDLs start with ``DROP TABLE IF EXISTS`` ‚ûú the helper is\n",
    "    **idempotent** and can run safely on every DAG execution.\n",
    "    \"\"\"\n",
    "    log.info(\"üîß  (Re)creating typed temp tables ‚Ä¶\")\n",
    "\n",
    "    ddl_statements = [\n",
    "        # ------------------------------------------------------------------\n",
    "        # cnae / industry_classification\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_cnae CASCADE;\n",
    "        CREATE TABLE temp_cnae (\n",
    "            codigo     CHAR(7),\n",
    "            descricao  TEXT     NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "        # ------------------------------------------------------------------\n",
    "        # empresas / companies\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_empresas CASCADE;\n",
    "        CREATE TABLE temp_empresas (\n",
    "            cnpj_basico                 CHAR(8),\n",
    "            razao_social                TEXT       NOT NULL,\n",
    "            natureza_juridica           CHAR(4)    NOT NULL,\n",
    "            qualificacao_responsavel    CHAR(2)    NOT NULL,\n",
    "            capital_social              TEXT,\n",
    "            porte_empresa               CHAR(2),\n",
    "            ente_federativo_responsavel TEXT\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # estabelecimento / establishments\n",
    "        # ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "        DROP TABLE IF EXISTS temp_estabelecimento CASCADE;\n",
    "        CREATE TABLE temp_estabelecimento (\n",
    "            cnpj_basico               CHAR(8),\n",
    "            cnpj_ordem                CHAR(4),\n",
    "            cnpj_dv                   CHAR(2),\n",
    "            matriz_filial             CHAR(1),\n",
    "            nome_fantasia             TEXT,\n",
    "            situacao_cadastral        CHAR(2),\n",
    "            data_situacao_cadastral   TEXT,\n",
    "            motivo_situacao_cadastral CHAR(2),\n",
    "            nome_cidade_exterior      TEXT,\n",
    "            pais                      CHAR(3),\n",
    "            data_inicio_atividades    TEXT,\n",
    "            cnae_fiscal               CHAR(7),\n",
    "            cnae_fiscal_secundaria    TEXT,\n",
    "            tipo_logradouro           TEXT,\n",
    "            logradouro                TEXT,\n",
    "            numero                    VARCHAR(10),\n",
    "            complemento               TEXT,\n",
    "            bairro                    TEXT,\n",
    "            cep                       CHAR(8),\n",
    "            uf                        CHAR(2),\n",
    "            municipio                 CHAR(4),\n",
    "            ddd1                      CHAR(4),\n",
    "            telefone1                 CHAR(8),\n",
    "            ddd2                      CHAR(4),\n",
    "            telefone2                 CHAR(8),\n",
    "            ddd_fax                   CHAR(4),\n",
    "            fax                       CHAR(8),\n",
    "            correio_eletronico        TEXT,\n",
    "            situacao_especial         TEXT,\n",
    "            data_situacao_especial    TEXT\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # motivo / status_reason\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_motivo CASCADE;\n",
    "        CREATE TABLE temp_motivo (\n",
    "            codigo    CHAR(2),\n",
    "            descricao TEXT    NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # municipio / municipalities\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_municipio CASCADE;\n",
    "        CREATE TABLE temp_municipio (\n",
    "            codigo    CHAR(4),\n",
    "            descricao TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # natureza_juridica / legal_nature\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_natureza_juridica CASCADE;\n",
    "        CREATE TABLE temp_natureza_juridica (\n",
    "            codigo    CHAR(4) PRIMARY KEY,\n",
    "            descricao TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # pais / countries\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_pais CASCADE;\n",
    "        CREATE TABLE temp_pais (\n",
    "            codigo    CHAR(3) PRIMARY KEY,\n",
    "            descricao TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # qualificacao_socio / partner_qualification\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_qualificacao_socio CASCADE;\n",
    "        CREATE TABLE temp_qualificacao_socio (\n",
    "            codigo    CHAR(2) PRIMARY KEY,\n",
    "            descricao TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # simples / simple_tax_regime\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_simples CASCADE;\n",
    "        CREATE TABLE temp_simples (\n",
    "            cnpj_basico            CHAR(8),\n",
    "            opcao_simples          CHAR(1),\n",
    "            data_opcao_simples     DATE,\n",
    "            data_exclusao_simples  DATE,\n",
    "            opcao_mei              CHAR(1),\n",
    "            data_opcao_mei         DATE,\n",
    "            data_exclusao_mei      DATE\n",
    "        );\n",
    "        \"\"\",\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # socios_original / original_partners\n",
    "        # ------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        DROP TABLE IF EXISTS temp_socios_original CASCADE;\n",
    "        CREATE TABLE temp_socios_original (\n",
    "            cnpj_basico                      CHAR(8),\n",
    "            identificador_de_socio           CHAR(1),\n",
    "            nome_socio                       TEXT,\n",
    "            cnpj_cpf_socio                   CHAR(14),\n",
    "            qualificacao_socio               CHAR(2),\n",
    "            data_entrada_sociedade           DATE,\n",
    "            pais                             CHAR(3),\n",
    "            representante_legal              CHAR(11),\n",
    "            nome_representante               TEXT,\n",
    "            qualificacao_representante_legal CHAR(2),\n",
    "            faixa_etaria                     CHAR(1)\n",
    "        );\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    pg = Postgres()\n",
    "    for ddl in ddl_statements:\n",
    "        log.debug(\"Executing DDL:\\n%s\", ddl.strip())\n",
    "        pg.execute_sql(ddl)\n",
    "\n",
    "    log.info(\"‚úÖ  Temp tables successfully (re)created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)  Low-level I/O ¬∑ robust COPY with encoding detection (UTF-8/LATIN-1) and disk-backed NUL-strip fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _has_nul_bytes(p: Path, probe_bytes: int = 64 * 1024) -> bool:\n",
    "    \"\"\"\n",
    "    Quickly probe for rogue NUL bytes in a CSV file.\n",
    "\n",
    "    Rationale\n",
    "    ---------\n",
    "    PostgreSQL COPY rejects NUL bytes. Scanning the entire file would be\n",
    "    wasteful, so we heuristically check only the first *probe_bytes*.\n",
    "    In practice, if NUL appears in the head, it tends to appear elsewhere.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Path\n",
    "        CSV file path to probe.\n",
    "    probe_bytes : int, default 64 KiB\n",
    "        Number of bytes to read from the head of the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True  -> at least one NUL ('\\\\x00') found in the probed slice.\n",
    "        False -> no NUL found in the slice (not a formal proof of absence).\n",
    "    \"\"\"\n",
    "    with p.open(\"rb\") as fh:\n",
    "        return b\"\\x00\" in fh.read(probe_bytes)\n",
    "\n",
    "def _detect_csv_encoding(p: Path, probe_bytes: int = 128 * 1024) -> str:\n",
    "    \"\"\"\n",
    "    Decide the best text encoding for a CSV file between **UTF-8** and **Latin-1**.\n",
    "\n",
    "    Strategy\n",
    "    --------\n",
    "    ‚Ä¢ Read only the first *probe_bytes* to decide quickly (no full scan).\n",
    "    ‚Ä¢ If there is a UTF-8 BOM (EF BB BF) ‚Üí return **'utf-8-sig'**.\n",
    "    ‚Ä¢ Else, try to decode that slice as UTF-8:\n",
    "        ‚Äì If it succeeds ‚Üí return **'utf-8'**.\n",
    "        ‚Äì If it raises UnicodeDecodeError ‚Üí fall back to **'latin-1'**.\n",
    "      (Latin-1 never raises on arbitrary bytes, so order matters.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Path\n",
    "        Path to the CSV file on disk.\n",
    "    probe_bytes : int, default 128 KiB\n",
    "        Number of bytes to sample from the file head.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        One of: **'utf-8-sig'**, **'utf-8'**, or **'latin-1'**.\n",
    "    \"\"\"\n",
    "    with p.open(\"rb\") as fh:\n",
    "        head = fh.read(probe_bytes)\n",
    "\n",
    "    # BOM present ‚Üí UTF-8 with signature\n",
    "    if head.startswith(b\"\\xEF\\xBB\\xBF\"):\n",
    "        return \"utf-8-sig\"\n",
    "\n",
    "    # Try a plain UTF-8 decode on the sample\n",
    "    try:\n",
    "        head.decode(\"utf-8\")\n",
    "        return \"utf-8\"\n",
    "    except UnicodeDecodeError:\n",
    "        return \"latin-1\"\n",
    "\n",
    "\n",
    "def _copy_csv_to_pg(csv_path: Path, target_table: str) -> None:\n",
    "    \"\"\"\n",
    "    Stream **one** CSV into PostgreSQL via `COPY ‚Ä¶ FROM STDIN` (CSV mode),\n",
    "    with **per-file encoding detection** and a **NUL-strip** disk-backed fallback.\n",
    "\n",
    "    Robustness & memory-safety\n",
    "    --------------------------\n",
    "    1) **Encoding detection per file** (UTF-8 or Latin-1) to avoid mojibake\n",
    "       in the dimension tables (e.g., acentos).\n",
    "    2) **Fast path**: stream the file with the detected encoding directly\n",
    "       into `COPY` (low memory, no materialization).\n",
    "    3) **NUL handling**: if COPY fails and the error/probe suggests NUL bytes,\n",
    "       write a **temporary cleaned file** stripping only `\\\\x00` line-by-line,\n",
    "       then COPY from it. Avoids building giant in-memory buffers.\n",
    "    4) **Per-table CSV tweaks**: use `FORCE_NULL` on known date-like columns\n",
    "       so that `\"\"` becomes SQL NULL *before* casting happens downstream.\n",
    "\n",
    "    Receita CNPJ CSV specifics\n",
    "    --------------------------\n",
    "    ‚Ä¢ Delimiter: `;`\n",
    "    ‚Ä¢ No header row (HEADER false)\n",
    "    ‚Ä¢ Empty strings `\"\"` ‚Üí NULL (NULL '')\n",
    "    ‚Ä¢ Some files are UTF-8; others Latin-1 (ISO-8859-1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : Path\n",
    "        Absolute path to the CSV file to ingest.\n",
    "    target_table : str\n",
    "        Destination temp table name (e.g., `'temp_empresas'`).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        Any non-NUL related COPY error; or a failure that persists even\n",
    "        after the NUL-strip fallback.\n",
    "    \"\"\"\n",
    "    # ‚îÄ‚îÄ Per-table CSV options kept local (encapsulation) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    _COPY_PER_TABLE_OPTS = {\n",
    "        \"temp_estabelecimento\": (\n",
    "            \"FORCE_NULL (data_situacao_cadastral, data_inicio_atividades, data_situacao_especial)\"\n",
    "        ),\n",
    "        \"temp_simples\": (\n",
    "            \"FORCE_NULL (data_opcao_simples, data_exclusao_simples, data_opcao_mei, data_exclusao_mei)\"\n",
    "        ),\n",
    "        \"temp_socios_original\": \"FORCE_NULL (data_entrada_sociedade)\",\n",
    "        # add others if new date-like TEXT columns appear\n",
    "    }\n",
    "\n",
    "    # 0) Detect per-file encoding and map to Postgres client_encoding\n",
    "    file_encoding = _detect_csv_encoding(csv_path)\n",
    "    client_enc   = \"UTF8\" if file_encoding.startswith(\"utf-\") else \"LATIN1\"\n",
    "\n",
    "    pg   = Postgres()                     # thin wrapper (env via dotenv)\n",
    "    conn = pg.connect_postgres()\n",
    "    cur  = conn.cursor()\n",
    "\n",
    "    # Session knobs for predictable COPY STDIN behavior\n",
    "    cur.execute(f\"SET client_encoding TO '{client_enc}';\")   # COPY STDIN uses session encoding\n",
    "    cur.execute(\"SET datestyle TO ISO, YMD;\")                # predictable date parsing downstream\n",
    "\n",
    "    base_opts  = \"FORMAT csv, DELIMITER ';', NULL '', HEADER false\"\n",
    "    extra_opts = _COPY_PER_TABLE_OPTS.get(target_table, \"\")\n",
    "    copy_sql   = f\"COPY {target_table} FROM STDIN WITH ({base_opts}{', ' + extra_opts if extra_opts else ''})\"\n",
    "\n",
    "    try:\n",
    "        # ‚îÄ‚îÄ 1) Fast path ‚Äî raw stream in detected text encoding ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        with csv_path.open(\"r\", encoding=file_encoding, errors=\"replace\", newline=\"\") as fh:\n",
    "            cur.copy_expert(copy_sql, fh)\n",
    "        conn.commit()\n",
    "        log.debug(\"‚úì raw COPY (%s)  %s ‚Üí %s\", file_encoding, csv_path.name, target_table)\n",
    "        return\n",
    "\n",
    "    except Exception as exc_raw:\n",
    "        # ‚îÄ‚îÄ 2) Decide whether we should attempt the NUL-cleaning fallback ‚îÄ‚îÄ\n",
    "        conn.rollback()\n",
    "        msg = str(exc_raw).lower()\n",
    "        nul_suspect = (\"nul\" in msg) or (\"invalid byte\" in msg) or _has_nul_bytes(csv_path)\n",
    "\n",
    "        if not nul_suspect:\n",
    "            # Not a NUL/low-level byte issue ‚Üí surface the real problem\n",
    "            log.error(\"‚ùå COPY failed for %s (non-NUL error) ‚Äì %s\", csv_path.name, exc_raw)\n",
    "            raise\n",
    "\n",
    "        log.warning(\"COPY failed for %s (%s). Retrying with NUL-stripped temp file‚Ä¶\",\n",
    "                    csv_path.name, exc_raw)\n",
    "\n",
    "        # ‚îÄ‚îÄ 3) Disk-backed cleaning pass (constant memory) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(\"w+\", encoding=file_encoding, delete=False) as tmp:\n",
    "            tmp_path = Path(tmp.name)\n",
    "\n",
    "            # Stream read ‚Üí strip only NULs ‚Üí stream write\n",
    "            with csv_path.open(\"r\", encoding=file_encoding, errors=\"replace\", newline=\"\") as src:\n",
    "                for line in src:\n",
    "                    tmp.write(line.replace(\"\\x00\", \"\"))\n",
    "\n",
    "            tmp.flush()\n",
    "            tmp.seek(0)\n",
    "\n",
    "            try:\n",
    "                cur.copy_expert(copy_sql, tmp)\n",
    "                conn.commit()\n",
    "                log.debug(\"‚úì cleaned COPY (%s)  %s ‚Üí %s\", file_encoding, csv_path.name, target_table)\n",
    "            except Exception as exc_clean:\n",
    "                conn.rollback()\n",
    "                log.error(\"‚ùå COPY still failing for %s after NUL-strip ‚Äì %s\",\n",
    "                          csv_path.name, exc_clean)\n",
    "                raise\n",
    "            finally:\n",
    "                # Best-effort cleanup\n",
    "                try:\n",
    "                    tmp_path.unlink(missing_ok=True)\n",
    "                except Exception:\n",
    "                    log.warning(\"‚ö†Ô∏è Could not delete temp file %s\", tmp_path)\n",
    "\n",
    "    finally:\n",
    "        # ‚îÄ‚îÄ 4) Always close resources ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        try:\n",
    "            cur.close()\n",
    "        finally:\n",
    "            conn.close()\n",
    "        log.debug(\"Closed cursor/connection for %s\", csv_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)  High-level loader ¬∑ iterate CSV_DIR and COPY into matching temp tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 5 b)  High-level CSV loader  ‚Äì iterates over CSV_DIR\n",
    "# ------------------------------------------------------------------\n",
    "from typing import Optional\n",
    "\n",
    "def load_csvs_into_postgres(limit_files: Optional[int] = None) -> bool:\n",
    "    \"\"\"\n",
    "    COPY every ``*.csv`` in ``CSV_DIR`` into its matching *temp_* table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    limit_files : int | None, default **None**\n",
    "        ‚Ä¢ *None*  ‚Üí ingest **all** CSVs present  \n",
    "        ‚Ä¢ *N > 0* ‚Üí stop after N files (great for quick tests / CI)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        **True**  ‚Üí *all* loads succeeded  \n",
    "        **False** ‚Üí at least one file failed (check the logs)\n",
    "    \"\"\"\n",
    "    log.info(\n",
    "        \"üîÑ  CSV ‚Üí Postgres loader started  ‚Ä¢  limit=%s\",\n",
    "        limit_files if limit_files is not None else \"‚àû\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Collect CSV paths (alphabetical order for reproducibility)\n",
    "    # ------------------------------------------------------------------\n",
    "    csv_paths = sorted(p for p in CSV_DIR.iterdir() if p.suffix.lower() == \".csv\")\n",
    "    if limit_files is not None:\n",
    "        csv_paths = csv_paths[:limit_files]\n",
    "\n",
    "    if not csv_paths:\n",
    "        log.warning(\"‚ö†Ô∏è  No CSV files found in %s ‚Äì nothing to load.\", CSV_DIR)\n",
    "        return False\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Loop & COPY\n",
    "    # ------------------------------------------------------------------\n",
    "    all_ok   = True\n",
    "    processed = 0\n",
    "\n",
    "    for csv_path in csv_paths:\n",
    "        # (a) Which layout/table does this file belong to?\n",
    "        key = layout_key(csv_path.name)\n",
    "        if not key:\n",
    "            log.error(\"‚ùå  Could not infer layout for %s ‚Äì skipped.\", csv_path.name)\n",
    "            all_ok = False\n",
    "            continue\n",
    "\n",
    "        target_table = f\"temp_{key}\"\n",
    "\n",
    "        # (b) COPY into Postgres via our low-level helper\n",
    "        try:\n",
    "            _copy_csv_to_pg(csv_path, target_table)\n",
    "            processed += 1\n",
    "        except Exception as err:            # already partially logged inside helper\n",
    "            log.error(\"‚ùå  Load failed for %s ‚Äì %s\", csv_path.name, err)\n",
    "            all_ok = False\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Summary\n",
    "    # ------------------------------------------------------------------\n",
    "    log.info(\n",
    "        \"üèÅ  Loader finished ‚Äì %s file(s) processed ‚Äì status: %s\",\n",
    "        processed,\n",
    "        \"SUCCESS\" if all_ok else \"PARTIAL FAILURE\",\n",
    "    )\n",
    "    return all_ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7)  Promote temp tables ‚Üí final names (safer & noisier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6 b)  Stand-alone promotion helper\n",
    "# ------------------------------------------------------------------\n",
    "def promote_temp_tables(status: Optional[bool] = None, **ctx) -> None:\n",
    "    \"\"\"\n",
    "    Promote every *temp_* table to its final name ‚Äî but only if the CSV loader\n",
    "    succeeded.\n",
    "\n",
    "    Works in two modes:\n",
    "      ‚Ä¢ Airflow task: don't pass `status`; we read XCom from `load_csvs`.\n",
    "      ‚Ä¢ Notebook / script: pass `status=True/False` explicitly.\n",
    "\n",
    "    Safety rails\n",
    "    ------------\n",
    "    ‚Ä¢ Status gate: if status is falsy ‚Üí abort promotion.\n",
    "    ‚Ä¢ Best-effort: each table is attempted independently.\n",
    "    ‚Ä¢ Idempotent: we DROP the final table (if exists) before RENAME.\n",
    "    ‚Ä¢ Missing temp_*: logged as 'skipped'.\n",
    "    \"\"\"\n",
    "    # 1) Resolve the loader status\n",
    "    if status is None:\n",
    "        # Airflow path: try to read from XCom, default to False if missing\n",
    "        try:\n",
    "            ti = ctx.get(\"ti\")\n",
    "            if ti is not None:\n",
    "                status = bool(ti.xcom_pull(task_ids=\"load_csvs\", key=\"status\"))\n",
    "            else:\n",
    "                status = False\n",
    "        except Exception:\n",
    "            status = False\n",
    "\n",
    "    if not status:\n",
    "        log.error(\"üö´  CSV loader reported errors ‚Äì temp-table promotion ABORTED.\")\n",
    "        return\n",
    "\n",
    "    # 2) Promote table-by-table\n",
    "    pg = Postgres()\n",
    "    promoted = 0\n",
    "    skipped  = 0\n",
    "    errors   = 0\n",
    "\n",
    "    for name in LAYOUTS.keys():\n",
    "        temp_name = f\"temp_{name}\"\n",
    "        log.info(\"üîÑ  Promoting  %s  ‚Üí  %s\", temp_name, name)\n",
    "\n",
    "        try:\n",
    "            # Try DROP final then RENAME temp->final.\n",
    "            # If temp_* doesn't exist, the ALTER will throw and we'll log as 'skipped'.\n",
    "            pg.execute_sql(f\"DROP TABLE IF EXISTS {name};\")\n",
    "            pg.execute_sql(f\"ALTER TABLE {temp_name} RENAME TO {name};\")\n",
    "            promoted += 1\n",
    "            log.debug(\"‚úì  %s promoted successfully\", name)\n",
    "\n",
    "        except Exception as exc:\n",
    "            msg = str(exc).lower()\n",
    "            if \"does not exist\" in msg or \"relation\" in msg and \"does not exist\" in msg:\n",
    "                # No temp_* for this layout this run ‚Äî just skip noisily\n",
    "                skipped += 1\n",
    "                log.warning(\"‚Ü™Ô∏è  Skipping %s ‚Äì %s\", name, exc)\n",
    "            else:\n",
    "                errors += 1\n",
    "                log.error(\"‚ùå  Failed promoting %s ‚Äì %s\", name, exc)\n",
    "\n",
    "    # 3) Summary\n",
    "    if errors == 0:\n",
    "        log.info(\"üéâ  Promotion finished ‚Äì %s swapped, %s skipped, %s errors.\",\n",
    "                 promoted, skipped, errors)\n",
    "    else:\n",
    "        log.warning(\"‚ö†Ô∏è  Promotion finished with %s error(s) ‚Äì %s swapped, %s skipped.\",\n",
    "                    errors, promoted, skipped)\n",
    "\n",
    "\n",
    "def _harden_final_schemas() -> None:\n",
    "    \"\"\"\n",
    "    Convert TEXT-like date columns to proper DATE in final tables, cleaning common garbage:\n",
    "    \"\", \"0\", \"00000000\", non-digits. Safe & idempotent:\n",
    "      - Skips tables missing.\n",
    "      - Skips columns already DATE.\n",
    "      - For text-ish columns, casts using a robust CASE over col::text.\n",
    "    \"\"\"\n",
    "    pg = Postgres()\n",
    "    conn = pg.connect_postgres()\n",
    "    cur  = conn.cursor()\n",
    "\n",
    "    targets = {\n",
    "        \"estabelecimento\": [\n",
    "            \"data_situacao_cadastral\",\n",
    "            \"data_inicio_atividades\",\n",
    "            \"data_situacao_especial\",\n",
    "        ],\n",
    "        \"simples\": [\n",
    "            \"data_opcao_simples\",\n",
    "            \"data_exclusao_simples\",\n",
    "            \"data_opcao_mei\",\n",
    "            \"data_exclusao_mei\",\n",
    "        ],\n",
    "        \"socios_original\": [\n",
    "            \"data_entrada_sociedade\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def table_exists(tbl: str) -> bool:\n",
    "        cur.execute(\"SELECT to_regclass('public.' || %s) IS NOT NULL;\", (tbl,))\n",
    "        return bool(cur.fetchone()[0])\n",
    "\n",
    "    def column_type(tbl: str, col: str) -> str | None:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT data_type\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema='public' AND table_name=%s AND column_name=%s\n",
    "            \"\"\",\n",
    "            (tbl, col),\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        return row[0] if row else None  # e.g. 'text', 'character varying', 'date', ...\n",
    "\n",
    "    # Expression that normalizes a TEXT-ish column into DATE.\n",
    "    # Note: we always operate on col::text to support current type TEXT or DATE.\n",
    "    def cast_expr(col: str) -> str:\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "          WHEN NULLIF(regexp_replace(({col})::text, '\\\\D', '', 'g'), '') IS NULL\n",
    "               OR NULLIF(regexp_replace(({col})::text, '\\\\D', '', 'g'), '') = '0'\n",
    "            THEN NULL\n",
    "          WHEN length(regexp_replace(({col})::text, '\\\\D', '', 'g')) = 8\n",
    "            THEN to_date(regexp_replace(({col})::text, '\\\\D', '', 'g'), 'YYYYMMDD')\n",
    "          WHEN ({col})::text ~ '^\\\\d{{4}}-\\\\d{{2}}-\\\\d{{2}}$'\n",
    "            THEN (({col})::text)::date\n",
    "          ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "\n",
    "    altered_any = False\n",
    "\n",
    "    for tbl, cols in targets.items():\n",
    "        if not table_exists(tbl):\n",
    "            log.info(\"‚Ü™Ô∏è  Skip harden: table %s does not exist.\", tbl)\n",
    "            continue\n",
    "\n",
    "        alters = []\n",
    "        for col in cols:\n",
    "            ctype = column_type(tbl, col)\n",
    "            if ctype is None:\n",
    "                log.info(\"‚Ü™Ô∏è  Skip column: %s.%s not found.\", tbl, col)\n",
    "                continue\n",
    "            if ctype.lower() == \"date\":\n",
    "                log.debug(\"‚Ä¢ %s.%s already DATE ‚Äî skipping.\", tbl, col)\n",
    "                continue\n",
    "\n",
    "            # Only non-DATE columns are altered\n",
    "            alters.append(f\"ALTER COLUMN {col} TYPE DATE USING {cast_expr(col)}\")\n",
    "\n",
    "        if alters:\n",
    "            sql = f\"ALTER TABLE {tbl} \" + \", \".join(alters) + \";\"\n",
    "            cur.execute(sql)\n",
    "            cur.execute(f\"ANALYZE {tbl};\")\n",
    "            conn.commit()\n",
    "            altered_any = True\n",
    "            log.info(\"‚úÖ Hardened %s (%d column(s)).\", tbl, len(alters))\n",
    "        else:\n",
    "            log.info(\"‚Ü™Ô∏è  Nothing to harden on %s.\", tbl)\n",
    "\n",
    "    if altered_any:\n",
    "        log.info(\"üß± Final schemas hardened (TEXT‚ÜíDATE with cleanup).\")\n",
    "    else:\n",
    "        log.info(\"‚ÑπÔ∏è  No columns altered (already consistent).\")\n",
    "\n",
    "    try:\n",
    "        cur.close()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú üîß  ZIP ‚ûú CSV extraction started (archives=10)\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [1/10]  Cnaes.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [2/10]  Empresas0.zip\n",
      "21:58:42  ERROR     nb-cnpj-ingest ‚ûú ‚ùå  Bad ZIP Empresas0.zip ‚Äì File is not a zip file\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [3/10]  Estabelecimentos0.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [4/10]  Motivos.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [5/10]  Municipios.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [6/10]  Naturezas.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [7/10]  Paises.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [8/10]  Qualificacoes.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [9/10]  Simples.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú ‚Üí [10/10]  Socios0.zip\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú    ‚úî  done in 0.00s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú üèÅ  Extraction finished: 9 / 10 archive(s) processed in 0.01s\n",
      "21:58:42  INFO      nb-cnpj-ingest ‚ûú üîß  (Re)creating typed temp tables ‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Running: extract_zip_to_csv ‚Üí create_temp_tables ‚Üí load_csvs ‚Üí promote ‚Üí harden\n",
      "\n",
      "1Ô∏è‚É£  Extracting ZIPs‚Ä¶\n",
      "\n",
      "2Ô∏è‚É£  Creating temp tables‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "21:58:43  INFO      nb-cnpj-ingest ‚ûú ‚úÖ  Temp tables successfully (re)created.\n",
      "21:58:43  INFO      nb-cnpj-ingest ‚ûú üîÑ  CSV ‚Üí Postgres loader started  ‚Ä¢  limit=‚àû\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£  Loading CSVs into Postgres‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "22:02:19  ERROR     nb-cnpj-ingest ‚ûú ‚ùå COPY failed for Simples_F.K03200$W.SIMPLES.CSV.D51011.csv (non-NUL error) ‚Äì date/time field value out of range: \"00000000\"\n",
      "CONTEXT:  COPY temp_simples, line 2, column data_opcao_mei: \"00000000\"\n",
      "\n",
      "22:02:19  ERROR     nb-cnpj-ingest ‚ûú ‚ùå  Load failed for Simples_F.K03200$W.SIMPLES.CSV.D51011.csv ‚Äì date/time field value out of range: \"00000000\"\n",
      "CONTEXT:  COPY temp_simples, line 2, column data_opcao_mei: \"00000000\"\n",
      "\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "22:02:46  INFO      nb-cnpj-ingest ‚ûú üèÅ  Loader finished ‚Äì 8 file(s) processed ‚Äì status: PARTIAL FAILURE\n",
      "22:02:46  ERROR     nb-cnpj-ingest ‚ûú üö´  CSV loader reported errors ‚Äì temp-table promotion ABORTED.\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "22:02:46  INFO      nb-cnpj-ingest ‚ûú ‚Ü™Ô∏è  Nothing to harden on estabelecimento.\n",
      "22:02:46  INFO      nb-cnpj-ingest ‚ûú ‚Ü™Ô∏è  Nothing to harden on simples.\n",
      "22:02:46  INFO      nb-cnpj-ingest ‚ûú ‚Ü™Ô∏è  Nothing to harden on socios_original.\n",
      "22:02:46  INFO      nb-cnpj-ingest ‚ûú ‚ÑπÔ∏è  No columns altered (already consistent).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ü≥ load status: PARTIAL FAILURE\n",
      "\n",
      "4Ô∏è‚É£  Promoting temp_* ‚Üí final‚Ä¶\n",
      "\n",
      "5Ô∏è‚É£  Hardening final schemas (TEXT‚ÜíDATE)‚Ä¶\n",
      "\n",
      "‚úÖ Done in 243.9s.\n"
     ]
    }
   ],
   "source": [
    "# ‚ñ∂ (extract ‚Üí DDL ‚Üí load ‚Üí promote ‚Üí harden)\n",
    "from types import SimpleNamespace\n",
    "import time\n",
    "\n",
    "print(\"\\nüß© Running: extract_zip_to_csv ‚Üí create_temp_tables ‚Üí load_csvs ‚Üí promote ‚Üí harden\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# 1) Extract ZIP ‚Üí CSV\n",
    "print(\"\\n1Ô∏è‚É£  Extracting ZIPs‚Ä¶\")\n",
    "extract_zip_to_csv(limit=None)\n",
    "\n",
    "# 2) (Re)create temp tables\n",
    "print(\"\\n2Ô∏è‚É£  Creating temp tables‚Ä¶\")\n",
    "create_temp_tables()\n",
    "\n",
    "# 3) Load all CSVs into temp_* (us3 layout_key + COPY robust)\n",
    "print(\"\\n3Ô∏è‚É£  Loading CSVs into Postgres‚Ä¶\")\n",
    "status = load_csvs_into_postgres(limit_files=None)\n",
    "print(f\"   ‚Ü≥ load status: {'SUCCESS' if status else 'PARTIAL FAILURE'}\")\n",
    "\n",
    "# 4) Promote temp_* ‚Üí final\n",
    "print(\"\\n4Ô∏è‚É£  Promoting temp_* ‚Üí final‚Ä¶\")\n",
    "fake_ctx = {\"ti\": SimpleNamespace(xcom_pull=lambda **kw: status)}\n",
    "promote_temp_tables(status=status)\n",
    "\n",
    "# 5) Harden final schemas (TEXT ‚Üí DATE)\n",
    "print(\"\\n5Ô∏è‚É£  Hardening final schemas (TEXT‚ÜíDATE)‚Ä¶\")\n",
    "_harden_final_schemas()\n",
    "\n",
    "print(f\"\\n‚úÖ Done in {time.perf_counter() - t0:.1f}s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:02:46  INFO      Using ZIP: Estabelecimentos0.zip (1.73 GB)\n",
      "22:02:46  INFO      estab0-loader ‚ûú Using ZIP: Estabelecimentos0.zip (1.73 GB)\n",
      "22:02:46  INFO      Extracting K3241.K03200Y0.D51011.ESTABELE ‚Üí Estabelecimentos0__K3241.K03200Y0.D51011.ESTABELE\n",
      "22:02:46  INFO      estab0-loader ‚ûú Extracting K3241.K03200Y0.D51011.ESTABELE ‚Üí Estabelecimentos0__K3241.K03200Y0.D51011.ESTABELE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:02:59  INFO      Target CSV: Estabelecimentos0__K3241.K03200Y0.D51011.ESTABELE (5.51 GB)\n",
      "22:02:59  INFO      estab0-loader ‚ûú Target CSV: Estabelecimentos0__K3241.K03200Y0.D51011.ESTABELE (5.51 GB)\n",
      "22:03:00  INFO      Preview L1: \"11779918\";\"0001\";\"05\";\"1\";\"\";\"04\";\"20210310\";\"63\";\"\";\"\";\"20100312\";\"5611204\";\"5611203\";\"RUA\";\"ANA NERI\";\"73\";\"\";\"CAMBUI\";\"13024500\";\"SP\";\"6291\";\"19\";\"91174491\";\"\";\"\";\"\";\"\";\"kaconori@ig.com.br\";\"\";\"\"\n",
      "22:03:00  INFO      estab0-loader ‚ûú Preview L1: \"11779918\";\"0001\";\"05\";\"1\";\"\";\"04\";\"20210310\";\"63\";\"\";\"\";\"20100312\";\"5611204\";\"5611203\";\"RUA\";\"ANA NERI\";\"73\";\"\";\"CAMBUI\";\"13024500\";\"SP\";\"6291\";\"19\";\"91174491\";\"\";\"\";\"\";\"\";\"kaconori@ig.com.br\";\"\";\"\"\n",
      "22:03:00  INFO      Preview L2: \"11779934\";\"0001\";\"90\";\"1\";\"JOIAS VIP\";\"08\";\"20120927\";\"01\";\"\";\"\";\"20100325\";\"4783101\";\"4783102,4618499\";\"RUA\";\"PRESIDENTE FARIA\";\"282\";\"ANDAR 3                   SALA  303\";\"CENTRO\";\"80020290\";\"PR\";\"7535\";\"41\";\"31112626\";\"\";\"\";\"41\";\"31112626\";\"\";\"\";\"\"\n",
      "22:03:00  INFO      estab0-loader ‚ûú Preview L2: \"11779934\";\"0001\";\"90\";\"1\";\"JOIAS VIP\";\"08\";\"20120927\";\"01\";\"\";\"\";\"20100325\";\"4783101\";\"4783102,4618499\";\"RUA\";\"PRESIDENTE FARIA\";\"282\";\"ANDAR 3                   SALA  303\";\"CENTRO\";\"80020290\";\"PR\";\"7535\";\"41\";\"31112626\";\"\";\"\";\"41\";\"31112626\";\"\";\"\";\"\"\n",
      "WARNING:  database \"airflow\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.41.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE airflow REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "22:06:30  INFO      COPY done (stream LATIN1 + NUL-strip) in 210.6s\n",
      "22:06:30  INFO      estab0-loader ‚ûú COPY done (stream LATIN1 + NUL-strip) in 210.6s\n",
      "22:07:22  INFO      Row count now in temp_estabelecimento: 50535942\n",
      "22:07:22  INFO      estab0-loader ‚ûú Row count now in temp_estabelecimento: 50535942\n"
     ]
    }
   ],
   "source": [
    "# %% Load only Estabelecimentos0.zip ‚Üí temp_estabelecimento (binary COPY, robust logs)\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile, io, time, logging\n",
    "from postgre_connector import Postgres  # usa suas credenciais .env via helper\n",
    "\n",
    "# ‚îÄ‚îÄ repo/datasets detection (same logic used before) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "while REPO_ROOT != REPO_ROOT.parent:\n",
    "    if (REPO_ROOT / \"airflow\" / \"dags\").is_dir():\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "BASE_DIR = REPO_ROOT / \"airflow\" / \"datasets\"\n",
    "ZIP_DIR  = BASE_DIR / \"public-zips\"\n",
    "CSV_DIR  = BASE_DIR / \"public-data\"\n",
    "ZIP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ logging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "log = logging.getLogger(\"estab0-loader\")\n",
    "if not log.handlers:\n",
    "    h = logging.StreamHandler()\n",
    "    h.setFormatter(logging.Formatter(\"%(asctime)s  %(levelname)-8s  %(message)s\", \"%H:%M:%S\"))\n",
    "    log.addHandler(h)\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def extract_target_csv_from_zip(zpath: Path, out_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Extract the *largest* CSV inside the ZIP (typical Estabelecimentos0 has a single big CSV).\n",
    "    Returns the extracted path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zpath, \"r\") as z:\n",
    "        # list only CSV-like entries\n",
    "        members = [m for m in z.namelist() if m.lower().endswith(\".csv\") or \"estabele\" in m.lower()]\n",
    "        if not members:\n",
    "            raise FileNotFoundError(\"No CSV-like members found inside the ZIP.\")\n",
    "        # pick the largest by uncompressed size\n",
    "        info_by_name = {i.filename: i for i in z.infolist()}\n",
    "        target_name = max(members, key=lambda n: info_by_name[n].file_size)\n",
    "        tgt = out_dir / f\"{zpath.stem}__{Path(target_name).name if Path(target_name).suffix else Path(target_name).name + '.csv'}\"\n",
    "        if not tgt.exists():\n",
    "            log.info(\"Extracting %s ‚Üí %s\", target_name, tgt.name)\n",
    "            with z.open(target_name) as src, open(tgt, \"wb\") as dst:\n",
    "                # stream copy\n",
    "                while True:\n",
    "                    chunk = src.read(1 << 20)  # 1 MiB\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    dst.write(chunk)\n",
    "        else:\n",
    "            log.info(\"Using existing extracted file: %s\", tgt.name)\n",
    "        return tgt\n",
    "\n",
    "def copy_csv_binary(csv_path: Path, table: str = \"temp_estabelecimento\") -> None:\n",
    "    \"\"\"\n",
    "    COPY FROM STDIN robust:\n",
    "      - client_encoding LATIN1\n",
    "      - NULL '' (string vazia ‚Üí NULL)\n",
    "      - streaming com strip de NUL sem carregar tudo na mem√≥ria\n",
    "      - evita reinterpreta√ß√£o como UTF-8\n",
    "    Pr√©-requisito: staging sem FK e datas como TEXT.\n",
    "    \"\"\"\n",
    "    from postgre_connector import Postgres\n",
    "    import io\n",
    "\n",
    "    class TextStreamingCleaner:\n",
    "        \"\"\"Gerador de texto: l√™ bytes LATIN1 em blocos, remove NUL e entrega str.\"\"\"\n",
    "        def __init__(self, path: Path, chunk_bytes: int = 1 << 20):\n",
    "            self.path = path\n",
    "            self.chunk_bytes = chunk_bytes\n",
    "            self._fh = None\n",
    "        def __enter__(self):\n",
    "            self._fh = open(self.path, \"rb\")\n",
    "            return self\n",
    "        def __exit__(self, *exc):\n",
    "            try:\n",
    "                if self._fh: self._fh.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "        def read(self, n: int = -1) -> str:\n",
    "            if self._fh is None:\n",
    "                return \"\"\n",
    "            # l√™ um bloco bruto e converte para texto LATIN1, removendo NUL\n",
    "            raw = self._fh.read(self.chunk_bytes if n < 0 else n)\n",
    "            if not raw:\n",
    "                return \"\"\n",
    "            return raw.replace(b\"\\x00\", b\"\").decode(\"latin-1\", errors=\"replace\")\n",
    "\n",
    "    pg = Postgres()\n",
    "    conn = pg.connect_postgres()\n",
    "    cur  = conn.cursor()\n",
    "    try:\n",
    "        # Important√≠ssimo: diga ao servidor que STDIN vir√° em LATIN1\n",
    "        cur.execute(\"SET client_encoding TO 'LATIN1';\")\n",
    "        cur.execute(\"SET statement_timeout = 0;\")\n",
    "        cur.execute(\"SET synchronous_commit = off;\")\n",
    "\n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table}\n",
    "        FROM STDIN WITH (\n",
    "          FORMAT csv,\n",
    "          DELIMITER ';',\n",
    "          NULL '',\n",
    "          HEADER false\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        # Caminho 1: streaming textual (j√° decodificado em LATIN1)\n",
    "        t0 = time.time()\n",
    "        with TextStreamingCleaner(csv_path) as reader:\n",
    "            cur.copy_expert(copy_sql, reader)  # espera str ‚Üí respeita client_encoding\n",
    "        conn.commit()\n",
    "        log.info(\"COPY done (stream LATIN1 + NUL-strip) in %.1fs\", time.time() - t0)\n",
    "\n",
    "        # Confer√™ncia r√°pida\n",
    "        cur.execute(\"SELECT COUNT(*) FROM temp_estabelecimento;\")\n",
    "        rows = cur.fetchone()[0]\n",
    "        log.info(\"Row count now in temp_estabelecimento: %s\", rows)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            cur.close(); conn.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "       \n",
    "\n",
    "# ‚îÄ‚îÄ main: pick Estabelecimentos0.zip only, extract, COPY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "zfile = ZIP_DIR / \"Estabelecimentos0.zip\"\n",
    "if not zfile.exists():\n",
    "    raise FileNotFoundError(f\"{zfile} not found. Make sure Estabelecimentos0.zip is in {ZIP_DIR}\")\n",
    "\n",
    "if not zipfile.is_zipfile(zfile):\n",
    "    raise RuntimeError(f\"{zfile} is not a valid ZIP (might be a partial/HTML download).\")\n",
    "\n",
    "log.info(\"Using ZIP: %s (%.2f GB)\", zfile.name, zfile.stat().st_size / (1<<30))\n",
    "csv_path = extract_target_csv_from_zip(zfile, CSV_DIR)\n",
    "log.info(\"Target CSV: %s (%.2f GB)\", csv_path.name, csv_path.stat().st_size / (1<<30))\n",
    "\n",
    "# heads up: Receita CSV N√ÉO tem header; se o preview mostrar nomes de coluna, mude HEADER true\n",
    "try:\n",
    "    # quick preview\n",
    "    with csv_path.open(\"rb\") as fh:\n",
    "        head = fh.read(4096).decode(\"latin-1\", errors=\"replace\").splitlines()[:2]\n",
    "    log.info(\"Preview L1: %s\", head[0] if head else \"<empty>\")\n",
    "    log.info(\"Preview L2: %s\", head[1] if len(head) > 1 else \"<none>\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# execute COPY\n",
    "copy_csv_binary(csv_path, \"temp_estabelecimento\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
