{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (0)  Path & logging utilities (works everywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viniciuslamb/Documents/portifolio_cnpj/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# (0)  Path & logging utilities (works everywhere)\n",
    "# -----------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:35:06  INFO      setup ‚Üí üìÅ REPO_ROOT ‚ûú /Users/viniciuslamb/Documents/portifolio_cnpj\n",
      "20:35:06  INFO      setup ‚Üí üìÅ DATA_DIR  ‚ûú /Users/viniciuslamb/Documents/portifolio_cnpj/airflow/datasets/public-data\n",
      "20:35:06  INFO      setup ‚Üí üìÅ ZIP_DIR   ‚ûú /Users/viniciuslamb/Documents/portifolio_cnpj/airflow/datasets/public-zips\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "#  Path resolution & folder setup (robust & portable)\n",
    "#  -----------------------------------------------------------\n",
    "#  Works seamlessly in:\n",
    "#   1.  Plain Python scripts  ‚Üí where __file__ is defined\n",
    "#   2.  Jupyter / IPython notebooks ‚Üí uses current working dir\n",
    "#   3.  Docker containers with mounted volumes\n",
    "#\n",
    "#  Defines the four canonical paths:\n",
    "#       REPO_ROOT   ‚Üí top-level project folder\n",
    "#       BASE_DIR    ‚Üí <repo>/airflow/datasets\n",
    "#       DATA_DIR    ‚Üí BASE_DIR/public-data\n",
    "#       ZIP_DIR     ‚Üí BASE_DIR/public-zips\n",
    "# ===========================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Locate the real project root by walking upward until we find\n",
    "# BOTH \"airflow/dags\" and \"streamlit_app\" ‚Äî prevents false matches\n",
    "# under subfolders like /notebooks.\n",
    "# -----------------------------------------------------------------\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "while REPO_ROOT != REPO_ROOT.parent:\n",
    "    has_airflow = (REPO_ROOT / \"airflow\" / \"dags\").is_dir()\n",
    "    has_streamlit = (REPO_ROOT / \"streamlit_app\").is_dir()\n",
    "    if has_airflow and has_streamlit:\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# Fail loudly if the expected structure isn't found\n",
    "if not ((REPO_ROOT / \"airflow\" / \"dags\").is_dir() and (REPO_ROOT / \"streamlit_app\").is_dir()):\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå Could not locate project root (expected both 'airflow/dags' and 'streamlit_app'). \"\n",
    "        \"Run this script from inside the repository tree.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Define canonical dataset directories under Airflow\n",
    "# -----------------------------------------------------------------\n",
    "BASE_DIR = REPO_ROOT / \"airflow\" / \"datasets\"\n",
    "DATA_DIR = BASE_DIR / \"public-data\"\n",
    "ZIP_DIR  = BASE_DIR / \"public-zips\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Ensure the folders exist (both locally and inside Docker)\n",
    "# -----------------------------------------------------------------\n",
    "for folder in (DATA_DIR, ZIP_DIR):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Basic logging for visibility\n",
    "# -----------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s  %(levelname)-8s  %(name)s ‚Üí %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"setup\")\n",
    "log.info(\"üìÅ REPO_ROOT ‚ûú %s\", REPO_ROOT)\n",
    "log.info(\"üìÅ DATA_DIR  ‚ûú %s\", DATA_DIR)\n",
    "log.info(\"üìÅ ZIP_DIR   ‚ûú %s\", ZIP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  Folder maintenance helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  Folder-maintenance helper\n",
    "#  -----------------------------------------------------------\n",
    "#  *One* reusable function that wipes the contents of both\n",
    "#  `DATA_DIR`  (processed CSV / parquet) **and**\n",
    "#  `ZIP_DIR`   (raw .zip archives).\n",
    "#\n",
    "#  ‚ö†Ô∏è  IMPORTANT\n",
    "#  ---------------\n",
    "#  ‚Ä¢ This is destructive.  We therefore:\n",
    "#      ‚îÄ print a trash-can emoji üóëÔ∏è for every file removed\n",
    "#      ‚îÄ catch and report *permission* errors instead of crashing\n",
    "#  ‚Ä¢ No sub-directories are touched; only *direct* children that\n",
    "#    match the Unix glob pattern ‚Äú*‚Äù.\n",
    "# ===========================================================\n",
    "\n",
    "from typing import NoReturn\n",
    "\n",
    "def clean_folders() -> NoReturn:\n",
    "    \"\"\"\n",
    "    Purge previously-downloaded artefacts from *both* ``DATA_DIR`` and\n",
    "    ``ZIP_DIR``.\n",
    "\n",
    "    Behaviour\n",
    "    ---------\n",
    "    ‚Ä¢ Iterates over the two folders defined earlier.\n",
    "    ‚Ä¢ For *every* regular file encountered:\n",
    "        ‚Äì attempts ``Path.unlink()`` (i.e. delete)\n",
    "        ‚Äì prints a friendly message on success\n",
    "    ‚Ä¢ If a given file cannot be removed due to filesystem\n",
    "      permissions it prints a warning instead of raising.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NoReturn\n",
    "        The function exists solely for its side-effects\n",
    "        (deleted files + console output).\n",
    "    \"\"\"\n",
    "    for target_dir in (DATA_DIR, ZIP_DIR):\n",
    "        for candidate in target_dir.glob(\"*\"):\n",
    "            try:\n",
    "                candidate.unlink()\n",
    "                print(f\"üóëÔ∏è  Deleted {candidate.name}\")\n",
    "            except PermissionError:\n",
    "                print(f\"‚ö†Ô∏è  Permission denied ‚Üí {candidate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  Inventory helper ‚Äì what ZIPs do we already have?\n",
    "#  -----------------------------------------------------------\n",
    "#  Lists every file currently in `ZIP_DIR`, prints a neat table\n",
    "#  on screen **and** returns a dictionary for programmatic use.\n",
    "# ===========================================================\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from typing import Dict\n",
    "\n",
    "def inventory_existing_zips() -> Dict[str, dt]:\n",
    "    \"\"\"\n",
    "    Scan ``ZIP_DIR`` and build an inventory of already-downloaded\n",
    "    archives.\n",
    "\n",
    "    Console output\n",
    "    --------------\n",
    "    ‚Ä¢ If the directory is empty ‚Üí prints a single informative line.\n",
    "    ‚Ä¢ Otherwise prints **one line per file** with:\n",
    "        ‚îÄ filename (left-aligned)\n",
    "        ‚îÄ file size in MB (1 decimal)\n",
    "        ‚îÄ last-modified timestamp (local time)\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    dict[str, datetime]\n",
    "        Keys   ‚Üí *filename* (no path)\n",
    "        Values ‚Üí *mtime* (`datetime` object)\n",
    "\n",
    "    This is handy for downstream logic (e.g. ‚Äúskip files we already own‚Äù).\n",
    "    \"\"\"\n",
    "    # Quick exit: nothing to report\n",
    "    if not any(ZIP_DIR.iterdir()):\n",
    "        print(\"‚ÑπÔ∏è  No ZIP files found in\", ZIP_DIR)\n",
    "        return {}\n",
    "\n",
    "    print(\"üì¶  ZIP files currently present:\")\n",
    "    inventory: Dict[str, dt] = {}\n",
    "\n",
    "    # Iterate over direct children only (no recursion)\n",
    "    for file_path in ZIP_DIR.iterdir():\n",
    "        # Skip directories just in case\n",
    "        if file_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        modified     = dt.fromtimestamp(file_path.stat().st_mtime)\n",
    "        size_in_mb   = file_path.stat().st_size / 1_048_576\n",
    "        inventory[file_path.name] = modified\n",
    "\n",
    "        # Nicely formatted console row\n",
    "        print(\n",
    "            f\"  ‚Ä¢ {file_path.name:<50} \"\n",
    "            f\"{size_in_mb:6.1f} MB  ‚Äì  {modified:%Y-%m-%d %H:%M}\"\n",
    "        )\n",
    "\n",
    "    return inventory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)  Scrape the IRS (Receita) website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  Download helper ‚Äì discover the *latest* monthly folder and\n",
    "#  collect its .zip URLs\n",
    "# ===========================================================\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Constants\n",
    "# -----------------------------------------------------------------\n",
    "RECEITA_ROOT: str = (\n",
    "    \"https://arquivos.receitafederal.gov.br/cnpj/dados_abertos_cnpj/\"\n",
    ")\n",
    "HEADERS: dict[str, str] = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Main helper\n",
    "# -----------------------------------------------------------------\n",
    "def newest_month_links(max_links: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Crawl the Receita Federal ‚ÄúDados Abertos CNPJ‚Äù index and return\n",
    "    up to **`max_links`** direct `.zip` URLs belonging to the most\n",
    "    recent monthly folder (named `YYYYMM/`).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_links : int, default=3\n",
    "        Hard cap on how many archive links should be returned.\n",
    "        Handy for quick tests or CI jobs where bandwidth matters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list of fully-qualified URLs (https://‚Ä¶) ready for `requests`\n",
    "        or any download tool.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    ‚Ä¢ Uses a *very* lightweight scraping strategy ‚Äî BeautifulSoup +\n",
    "      simple CSS selectors.  \n",
    "    ‚Ä¢ Raises `RuntimeError` if no folders or no `.zip` files are found\n",
    "      (so callers can decide what to do).  \n",
    "    ‚Ä¢ Prints a concise progress report so notebook readers see exactly\n",
    "      what‚Äôs happening.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------\n",
    "    # Step 1 ‚Üí download the *main* index page\n",
    "    # ----------------------------------------------------------\n",
    "    print(\"üîé  Requesting main index ‚Ä¶\")\n",
    "    try:\n",
    "        index_resp = requests.get(RECEITA_ROOT, timeout=15, headers=HEADERS)\n",
    "        index_resp.raise_for_status()\n",
    "    except requests.RequestException as err:\n",
    "        raise RuntimeError(f\"Failed to fetch main index: {err}\") from err\n",
    "\n",
    "    soup_index = BeautifulSoup(index_resp.text, \"lxml\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Step 2 ‚Üí find every anchor that *starts* with ‚Äú20‚Äù (YYYYMM)\n",
    "    # ----------------------------------------------------------\n",
    "    month_folders = sorted(a[\"href\"] for a in soup_index.select('a[href^=\"20\"]'))\n",
    "    if not month_folders:\n",
    "        raise RuntimeError(\"Could not locate any monthly folders on the page.\")\n",
    "\n",
    "    latest_folder = month_folders[-1]           # e.g. \"202503/\"\n",
    "    month_url     = RECEITA_ROOT + latest_folder\n",
    "    print(f\"  ‚Ä¢ Latest folder detected: {latest_folder}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Step 3 ‚Üí open that folder and parse all `.zip` links\n",
    "    # ----------------------------------------------------------\n",
    "    try:\n",
    "        month_resp = requests.get(month_url, timeout=15, headers=HEADERS)\n",
    "        month_resp.raise_for_status()\n",
    "    except requests.RequestException as err:\n",
    "        raise RuntimeError(f\"Failed to open folder {latest_folder}: {err}\") from err\n",
    "\n",
    "    soup_month = BeautifulSoup(month_resp.text, \"lxml\")\n",
    "\n",
    "    # Build absolute URLs; keep only *.zip\n",
    "    zip_urls = [\n",
    "        href if href.startswith(\"http\") else month_url + href\n",
    "        for href in (a.get(\"href\") for a in soup_month.find_all(\"a\"))\n",
    "        if href and href.lower().endswith(\".zip\")\n",
    "    ]\n",
    "\n",
    "    if not zip_urls:\n",
    "        raise RuntimeError(f\"No .zip archives found under {latest_folder}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Step 4 ‚Üí trim to the requested limit & print preview\n",
    "    # ----------------------------------------------------------\n",
    "    capped_urls = zip_urls[:max_links]\n",
    "    print(f\"  ‚Ä¢ {len(zip_urls)} ZIPs available; returning the first {max_links}.\")\n",
    "    for idx, url in enumerate(capped_urls, 1):\n",
    "        print(f\"    - [{idx}] {Path(url).name}\")\n",
    "\n",
    "    return capped_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Download selected ZIP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  Download helper ‚Äì save each ZIP to disk (streaming + verify)\n",
    "# ===========================================================\n",
    "\n",
    "import time\n",
    "import zipfile\n",
    "from typing import List\n",
    "\n",
    "def download_zips(\n",
    "    urls: List[str],\n",
    "    destination: Path = ZIP_DIR,\n",
    "    chunk_size: int = 1 << 20,   # 1 MiB\n",
    "    max_retries: int = 3,        # validate + retry on corruption/network errors\n",
    "    retry_backoff_s: float = 2.0 # simple exponential backoff base\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Stream‚Äêdownload every archive in *urls* into *destination* and **validate**\n",
    "    the file as a real ZIP after each download. Corrupted or partial files are\n",
    "    deleted and re-downloaded automatically (up to `max_retries` times).\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    ‚Ä¢ Skips files that already exist **and** pass ZIP validation.  \n",
    "    ‚Ä¢ If a file exists but is invalid, it is removed and fetched again.  \n",
    "    ‚Ä¢ Streams in `chunk_size` blocks (memory-efficient).  \n",
    "    ‚Ä¢ Prints progress roughly every 5% when the server sends Content-Length.  \n",
    "    ‚Ä¢ Retries on network errors and on ZIP validation failures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list[str]\n",
    "        Fully-qualified HTTPS URLs pointing to `.zip` files.\n",
    "    destination : pathlib.Path\n",
    "        Folder where the archives will be saved.\n",
    "    chunk_size : int\n",
    "        Bytes per `iter_content` read.\n",
    "    max_retries : int\n",
    "        Maximum attempts per file (including the first try).\n",
    "    retry_backoff_s : float\n",
    "        Base seconds for exponential backoff between retries (2, 4, 8, ...).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if not urls:\n",
    "        print(\"‚ö†Ô∏è  No URLs supplied ‚Äì nothing to download.\")\n",
    "        return\n",
    "\n",
    "    destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(HEADERS)\n",
    "\n",
    "        total_files = len(urls)\n",
    "        for file_idx, url in enumerate(urls, start=1):\n",
    "            filename  = Path(url).name\n",
    "            local_zip = destination / filename\n",
    "\n",
    "            # If file already exists, validate it; skip only if valid.\n",
    "            if local_zip.exists():\n",
    "                if zipfile.is_zipfile(local_zip):\n",
    "                    print(f\"‚úÖ [{file_idx}/{total_files}] {filename} already present and valid ‚Äì skipping.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ôªÔ∏è  [{file_idx}/{total_files}] {filename} exists but is invalid ‚Äì re-downloading ‚Ä¶\")\n",
    "                    local_zip.unlink(missing_ok=True)\n",
    "\n",
    "            # Retry loop (covers both network errors and bad ZIPs)\n",
    "            attempt = 0\n",
    "            while attempt < max_retries:\n",
    "                attempt += 1\n",
    "                try:\n",
    "                    print(f\"‚¨áÔ∏è  [{file_idx}/{total_files}] Downloading {filename} (attempt {attempt}/{max_retries}) ‚Ä¶\")\n",
    "                    t0 = time.time()\n",
    "\n",
    "                    with session.get(url, stream=True, timeout=90) as resp:\n",
    "                        resp.raise_for_status()\n",
    "\n",
    "                        total_bytes   = int(resp.headers.get(\"content-length\", 0))\n",
    "                        next_progress = 5  # 5%, 10%, ‚Ä¶\n",
    "                        bytes_written = 0\n",
    "\n",
    "                        with local_zip.open(\"wb\") as fp:\n",
    "                            for chunk in resp.iter_content(chunk_size=chunk_size):\n",
    "                                if not chunk:\n",
    "                                    continue\n",
    "                                fp.write(chunk)\n",
    "                                bytes_written += len(chunk)\n",
    "                                if total_bytes:\n",
    "                                    pct = bytes_written * 100 / total_bytes\n",
    "                                    if pct >= next_progress:\n",
    "                                        mb = bytes_written / 1_048_576\n",
    "                                        print(f\"   ‚Ä¢ {pct:5.1f}% ({mb:,.1f} MB)\")\n",
    "                                        next_progress += 5\n",
    "\n",
    "                    elapsed = time.time() - t0\n",
    "                    print(f\"üéâ  Finished {filename} in {elapsed:,.1f}s\\n\")\n",
    "\n",
    "                    # ZIP integrity check\n",
    "                    if not zipfile.is_zipfile(local_zip):\n",
    "                        print(f\"‚ùå  Invalid ZIP detected for {filename}\")\n",
    "                        local_zip.unlink(missing_ok=True)\n",
    "                        if attempt < max_retries:\n",
    "                            sleep_s = retry_backoff_s ** attempt\n",
    "                            print(f\"   ‚Üª Retrying in {sleep_s:,.1f}s ‚Ä¶\")\n",
    "                            time.sleep(sleep_s)\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"üö´  Giving up on {filename} after {max_retries} attempts.\")\n",
    "                    # Valid file ‚Üí break retry loop\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                except requests.RequestException as err:\n",
    "                    print(f\"‚ùå  Network error while downloading {filename}: {err}\")\n",
    "                    local_zip.unlink(missing_ok=True)\n",
    "                    if attempt < max_retries:\n",
    "                        sleep_s = retry_backoff_s ** attempt\n",
    "                        print(f\"   ‚Üª Retrying in {sleep_s:,.1f}s ‚Ä¶\")\n",
    "                        time.sleep(sleep_s)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"üö´  Giving up on {filename} after {max_retries} attempts.\")\n",
    "\n",
    "    print(\"üèÅ  All downloads completed and validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "#  ‚ÄúOne-click‚Äù runner ‚Äì tie everything together\n",
    "# ===========================================================\n",
    "\n",
    "def run_cnpj_download(limit: int = 37, clean_first: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    End-to-end helper that:\n",
    "\n",
    "    1. *Optionally* wipes the target folders (`clean_first=True`);\n",
    "    2. Prints an inventory of ZIPs already on disk;\n",
    "    3. Scrapes the Receita Federal site for the **latest** monthly folder\n",
    "       and grabs up to `limit` `.zip` URLs;\n",
    "    4. Downloads any missing archives into `ZIP_DIR`; progress is streamed\n",
    "       to the notebook console;\n",
    "    5. Shows a final inventory so you can verify what was added.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    limit : int, default=3\n",
    "        Maximum number of archives to fetch.  Adjust to `None` or a\n",
    "        larger value if you want *all* ZIPs for the month.\n",
    "    clean_first : bool, default=False\n",
    "        If *True* the function will call :pyfunc:`clean_folders` at the\n",
    "        very start, guaranteeing a fresh state.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    ‚Ä¢ The individual steps are printed with emoji bullets so readers\n",
    "      can easily follow along.  \n",
    "    ‚Ä¢ All underlying helpers (scrape, download, inventory) already\n",
    "      include robust error handling; if something fails the exception\n",
    "      message will bubble up and the notebook cell will stop ‚Äî making\n",
    "      debugging straightforward.\n",
    "    \"\"\"\n",
    "    print(\"\\nüöÄ  CNPJ end-to-end download started\\n\" + \"-\" * 60)\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # (1) Optional cleanup\n",
    "    # ------------------------------------------------------\n",
    "    if clean_first:\n",
    "        print(\"üßπ  Cleaning existing files ‚Ä¶\")\n",
    "        clean_folders()\n",
    "        print(\"   Done. ‚ú®\\n\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # (2) Inventory before\n",
    "    # ------------------------------------------------------\n",
    "    print(\"üìë  Inventory *before* download:\")\n",
    "    pre_inventory = inventory_existing_zips()\n",
    "    print()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # (3) Scrape latest folder + collect links\n",
    "    # ------------------------------------------------------\n",
    "    try:\n",
    "        links = newest_month_links(max_links=limit)\n",
    "    except RuntimeError as err:\n",
    "        print(f\"‚ùå  Aborting ‚Äì {err}\")\n",
    "        return\n",
    "    print()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # (4) Download missing ZIPs\n",
    "    # ------------------------------------------------------\n",
    "    download_zips(links)\n",
    "    print()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # (5) Inventory after\n",
    "    # ------------------------------------------------------\n",
    "    print(\"üìë  Inventory *after* download:\")\n",
    "    post_inventory = inventory_existing_zips()\n",
    "    added = set(post_inventory) - set(pre_inventory)\n",
    "    print(f\"\\n‚úÖ  Added {len(added)} new file(s): {', '.join(added) if added else '-'}\")\n",
    "    print(\"\\nüèÅ  Workflow complete.\")\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è  Run with defaults (grab 3 ZIPs, keep existing files)\n",
    "#    Uncomment the line below inside your notebook cell.\n",
    "#run_cnpj_download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé  Requesting main index ‚Ä¶\n",
      "  ‚Ä¢ Latest folder detected: 2025-10/\n",
      "  ‚Ä¢ 37 ZIPs available; returning the first 5000.\n",
      "    - [1] Cnaes.zip\n",
      "    - [2] Empresas0.zip\n",
      "    - [3] Empresas1.zip\n",
      "    - [4] Empresas2.zip\n",
      "    - [5] Empresas3.zip\n",
      "    - [6] Empresas4.zip\n",
      "    - [7] Empresas5.zip\n",
      "    - [8] Empresas6.zip\n",
      "    - [9] Empresas7.zip\n",
      "    - [10] Empresas8.zip\n",
      "    - [11] Empresas9.zip\n",
      "    - [12] Estabelecimentos0.zip\n",
      "    - [13] Estabelecimentos1.zip\n",
      "    - [14] Estabelecimentos2.zip\n",
      "    - [15] Estabelecimentos3.zip\n",
      "    - [16] Estabelecimentos4.zip\n",
      "    - [17] Estabelecimentos5.zip\n",
      "    - [18] Estabelecimentos6.zip\n",
      "    - [19] Estabelecimentos7.zip\n",
      "    - [20] Estabelecimentos8.zip\n",
      "    - [21] Estabelecimentos9.zip\n",
      "    - [22] Motivos.zip\n",
      "    - [23] Municipios.zip\n",
      "    - [24] Naturezas.zip\n",
      "    - [25] Paises.zip\n",
      "    - [26] Qualificacoes.zip\n",
      "    - [27] Simples.zip\n",
      "    - [28] Socios0.zip\n",
      "    - [29] Socios1.zip\n",
      "    - [30] Socios2.zip\n",
      "    - [31] Socios3.zip\n",
      "    - [32] Socios4.zip\n",
      "    - [33] Socios5.zip\n",
      "    - [34] Socios6.zip\n",
      "    - [35] Socios7.zip\n",
      "    - [36] Socios8.zip\n",
      "    - [37] Socios9.zip\n",
      "üì¶ Local ZIPs: 9\n",
      "‚úÖ Families already present locally: cnae, empresas, motivo, municipio, natureza_juridica, pais, qualificacao_socio, simples, socios_original\n",
      "\n",
      "üîé Monthly links found: 37\n",
      "‚¨áÔ∏è Will download (one per family, only if missing locally): 1\n",
      "\n",
      " ‚Ä¢ estabelecimento        -> Estabelecimentos0.zip\n",
      "\n",
      "‚¨áÔ∏è Downloading selected ZIP(s)‚Ä¶\n",
      "\n",
      "‚¨áÔ∏è  [1/1] Downloading Estabelecimentos0.zip (attempt 1/3) ‚Ä¶\n",
      "   ‚Ä¢   5.0% (89.0 MB)\n",
      "   ‚Ä¢  10.0% (178.0 MB)\n",
      "   ‚Ä¢  15.0% (267.0 MB)\n",
      "   ‚Ä¢  20.0% (355.0 MB)\n",
      "   ‚Ä¢  25.0% (444.0 MB)\n",
      "   ‚Ä¢  30.0% (533.0 MB)\n",
      "   ‚Ä¢  35.0% (622.0 MB)\n",
      "   ‚Ä¢  40.0% (710.0 MB)\n",
      "   ‚Ä¢  45.0% (799.0 MB)\n",
      "   ‚Ä¢  50.0% (888.0 MB)\n",
      "   ‚Ä¢  55.0% (977.0 MB)\n",
      "   ‚Ä¢  60.0% (1,065.0 MB)\n",
      "   ‚Ä¢  65.0% (1,154.0 MB)\n",
      "   ‚Ä¢  70.0% (1,243.0 MB)\n",
      "   ‚Ä¢  75.0% (1,332.0 MB)\n",
      "   ‚Ä¢  80.0% (1,420.0 MB)\n",
      "   ‚Ä¢  85.0% (1,509.0 MB)\n",
      "   ‚Ä¢  90.0% (1,598.0 MB)\n",
      "   ‚Ä¢  95.1% (1,687.0 MB)\n",
      "   ‚Ä¢ 100.0% (1,774.8 MB)\n",
      "üéâ  Finished Estabelecimentos0.zip in 5,304.7s\n",
      "\n",
      "üèÅ  All downloads completed and validated.\n",
      "\n",
      "üß© Running: extract_zip_to_csv ‚Üí create_temp_tables ‚Üí load_csvs ‚Üí promote ‚Üí harden\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_zip_to_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müß© Running: extract_zip_to_csv ‚Üí create_temp_tables ‚Üí load_csvs ‚Üí promote ‚Üí harden\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 5.1) Extract whatever is in SOURCE_ZIP_DIR to CSV_DIR (idempotent; bad zips are skipped)\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mextract_zip_to_csv\u001b[49m(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 5.2) (Re)create temp tables\u001b[39;00m\n\u001b[1;32m    102\u001b[0m create_temp_tables()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_zip_to_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Notebook: pick-one-per-family (skip family if any local ZIP exists) + run pipeline\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# assumes you already have in the notebook:\n",
    "# newest_month_links, download_zips,\n",
    "# extract_zip_to_csv, create_temp_tables, load_csvs_into_postgres,\n",
    "# promote_temp_tables, _harden_final_schemas\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0) Resolve repo paths like in the DAG (no imports from your .py file)\n",
    "# ---------------------------------------------------------------------\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "while REPO_ROOT != REPO_ROOT.parent:\n",
    "    if (REPO_ROOT / \"airflow\" / \"dags\").is_dir():\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "BASE_DIR       = REPO_ROOT / \"airflow\" / \"datasets\"\n",
    "SOURCE_ZIP_DIR = BASE_DIR / \"public-zips\"\n",
    "CSV_DIR        = BASE_DIR / \"public-data\"\n",
    "SOURCE_ZIP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Fetch all links for the newest month (adjust the limit if needed)\n",
    "# ---------------------------------------------------------------------\n",
    "all_links = newest_month_links(max_links=5000)\n",
    "\n",
    "# family detection via filename substring (lower-cased)\n",
    "FAMILIES = {\n",
    "    \"header\": \"header\",\n",
    "    \"cnae\": \"cnae\",\n",
    "    \"empresas\": \"empresas\",\n",
    "    \"estabelec\": \"estabelecimento\",\n",
    "    \"motivo\": \"motivo\",\n",
    "    \"municipio\": \"municipio\",\n",
    "    \"natureza\": \"natureza_juridica\",\n",
    "    \"pais\": \"pais\",\n",
    "    \"qualific\": \"qualificacao_socio\",\n",
    "    \"simples\": \"simples\",\n",
    "    \"socios\": \"socios_original\",\n",
    "    \"secundaria\": \"cnaes_secundarias\",\n",
    "    \"trailler\": \"trailler\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Detect which families ALREADY have at least one local ZIP\n",
    "#    If a family is present, we won't download anything else for it.\n",
    "# ---------------------------------------------------------------------\n",
    "present_files = [p.name.lower() for p in SOURCE_ZIP_DIR.glob(\"*.zip\")]\n",
    "present_families = set()\n",
    "for fname in present_files:\n",
    "    for sub, fam in FAMILIES.items():\n",
    "        if sub in fname:\n",
    "            present_families.add(fam)\n",
    "            break\n",
    "\n",
    "print(f\"üì¶ Local ZIPs: {len(present_files)}\")\n",
    "if present_families:\n",
    "    print(\"‚úÖ Families already present locally:\", \", \".join(sorted(present_families)))\n",
    "else:\n",
    "    print(\"‚úÖ No local families yet.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) For each family NOT present, pick the first monthly link candidate\n",
    "# ---------------------------------------------------------------------\n",
    "chosen = {}  # family -> url\n",
    "for url in all_links:\n",
    "    name_l = Path(url).name.lower()\n",
    "    for sub, fam in FAMILIES.items():\n",
    "        if sub in name_l:\n",
    "            if fam in present_families or fam in chosen:\n",
    "                # already have a local ZIP for this family, or we already picked one to download\n",
    "                break\n",
    "            chosen[fam] = url\n",
    "            break\n",
    "\n",
    "print(f\"\\nüîé Monthly links found: {len(all_links)}\")\n",
    "print(f\"‚¨áÔ∏è Will download (one per family, only if missing locally): {len(chosen)}\\n\")\n",
    "for fam, url in sorted(chosen.items()):\n",
    "    print(f\" ‚Ä¢ {fam:<22} -> {Path(url).name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Download only what‚Äôs missing (no-op if everything is present)\n",
    "# ---------------------------------------------------------------------\n",
    "if chosen:\n",
    "    print(\"\\n‚¨áÔ∏è Downloading selected ZIP(s)‚Ä¶\\n\")\n",
    "    download_zips(list(chosen.values()))\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è Nothing to download (each family already has a local ZIP).\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Run the end-to-end pipeline using the functions already in the notebook\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"\\nüß© Running: extract_zip_to_csv ‚Üí create_temp_tables ‚Üí load_csvs ‚Üí promote ‚Üí harden\\n\")\n",
    "\n",
    "# 5.1) Extract whatever is in SOURCE_ZIP_DIR to CSV_DIR (idempotent; bad zips are skipped)\n",
    "extract_zip_to_csv(limit=None)\n",
    "\n",
    "# 5.2) (Re)create temp tables\n",
    "create_temp_tables()\n",
    "\n",
    "# 5.3) Load all CSVs (uses your layout_key + robust COPY)\n",
    "status = load_csvs_into_postgres(limit_files=None)\n",
    "print(f\"\\nüìä load_csvs_into_postgres status: {'SUCCESS' if status else 'PARTIAL FAILURE'}\")\n",
    "\n",
    "# 5.4) Promote temp_* ‚Üí final, honoring the status (emulates XCom in notebook)\n",
    "fake_ctx = {\"ti\": SimpleNamespace(xcom_pull=lambda **kw: status)}\n",
    "promote_temp_tables(**fake_ctx)\n",
    "\n",
    "# 5.5) Harden final schemas (TEXT ‚Üí DATE with cleanup)\n",
    "_harden_final_schemas()\n",
    "\n",
    "print(\"\\n‚úÖ Notebook pipeline finished.\")\n",
    "print(f\"   ZIPs dir: {SOURCE_ZIP_DIR}\")\n",
    "print(f\"   CSVs dir: {CSV_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
